![GenI-banner](https://github.com/genilab-fau/genilab-fau.github.io/blob/8d6ab41403b853a273983e4c06a7e52229f43df5/images/genilab-banner.png?raw=true)

# {Using Prompt Engineering for Enhancing Discord Bot Commands }

1-liner description of your project

This project explores the use of prompt engineering techniques to optimize and enhance the quality of responses generated by a Discord bot powered by a locally hosted LLaMA 3.2 model through Ollama. 

* Authors: [Ethan Curtis](https://github.com/basilicon), [Theodor Owchariw](https://github.com/TheodorOwchariw), [Jordan Perrone](https://github.com/jperrone27)
* Academic Supervisor: [Dr. Fernando Koch](http://www.fernandokoch.me)
  
# Research Question 

How can we use prompt engineering to improve the effectiveness of Discord bot commands? By leveraging structured prompting strategies and iterative improvements, we aim to enhance the bot’s ability to interpret and execute user commands efficiently. 

## Arguments

#### What is already known about this topic

* Various prompt engineering techniques exist, including: 
    * Zero-shot prompting: model generates responses without examples. 
    * Few-shot prompting: model uses limited examples for better context. 
    * Chain-of-thought (CoT): model follows step-by-step reasoning. 
    * Meta-prompting: dynamically refines and structures prompts. 
    * Self-consistency prompting: generates multiple answers and selects the most consistent one. 
* Hardcoded implementations of these techniques exist in our forked GitHub repository. 
* We aim to introduce one or two levels of automation, where the model iteratively refines prompts before generating a final response. 

#### What this research is exploring

* Evaluation of model parameters and prompting techniques to develop a Discord bot powered by LLaMA 3.2 running locally through Ollama. 
* Development of a structured pipeline that optimizes prompts for better response quality and appropriate actions by our Discord bot.  
* Investigation of key LLM parameters, including: 
    * Temperature: controls randomness in outputs. 
    * Context token limit: determines how much information the model considers. 
    * Max token length: affects response detail and completeness. 
    * Systematic tuning of parameters to balance coherence and accuracy.  

#### Implications for practice

* Enhancing the Discord bot’s ability to understand and execute natural language commands more effectively. 
*Improving task automation, information retrieval, and conversational assistance through structured prompting. 
* Providing users with more precise and contextually relevant responses for a smoother interaction experience. 

# Research Method

We developed a hybrid prompting technique which would be specific to our problem statement and conducted a series of tests to evaluate its effectiveness under varying conditions. To assess parameter sensitivity, we increased the values of key model parameters referring to context and output tokens by an order of magnitude (10x) individually and observed their effects on response quality. The temperature, on the other hand, was decreased to 0.1. 

Throughout this process, we monitored the generated responses and conducted qualitative evaluations to determine which configurations produced the most coherent and useful outputs. The assessment was performed through an anecdotal selection process, wherein we identified the responses that best aligned with the intended purpose of the bot. This approach allowed us to fine-tune the interaction between prompt engineering strategy and model parameter adjustments, ultimately leading to a more robust implementation. 

# Results

Our findings indicate that increasing context size significantly improves response quality, allowing the model to generate more detailed and contextually accurate outputs.  Through trial and error, it was found that creating a prompt that would generate a prompt like: “generate a prompt that would be considered simple for the model in hand (Llama 3.2 in this case)” resulted in far better answers. Next, it was found that changing the context to 10x after the new prompt has been generated results in an answer that was longer, more detailed, and objectively better overall. This would make sense as a larger context window would allow the model to find more details specific to the prompt. However, when doing this for the prompt generating prompt, it did not result in a better answer. This could mean that trying to use too much context to prompt generate can result in information overload and poor prompt response.  

The next parameter that was altered was temperature. This parameter was found to have a significant role in the answers that Llama would generate. Lowering the temperature after generating the new prompt resulted in an answer that is blunt yet very descriptive. This would be a result of creating answers that will not have much variation. Surprisingly, when the temperature decreased for the prompt generating prompt there was not much difference in answer output. This could suggest that there are a wide variety of prompts that can be generated that will result in similar responses from the model. The last parameter changed was increasing the number of predictions by 10x. It was found that doing this after the prompt is generated results in a response that is vaguer. This may be because with the increase of data it resulted in a more generalized response as a result of averaging the varying data. Contrasting with the prompt generating prompt, it results in the model’s response becoming more detailed, but not necessarily a better answer.  

# Further research

Although not immediately necessary for our Discord Bot use case, future research may involve experimenting with additional prompting techniques and automation levels. By further exploring different prompting methods at a higher level, it would result in an improved understanding of how prompting affects model output and better results overall. Additionally, evaluating user interactions in real-time to further refine prompt strategies based on feedback could enhance the bot’s adaptability and usability. Exploring the scalability of this approach for larger, more complex applications may also yield valuable insights into the broader implications of prompt engineering in AI-driven systems. 