{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Prompting\n",
    "\n",
    "A combination of several different kinds of prompting.  This is our own contribution, but it is a fairly obvious follow-up to the other prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': '\\nPlease return an JSON string using the following format:\\n{\\n\"result\": 200, // an HTTPS result, either 200 or 503.\\n\"formatted-query\": \"!help\" // PUT THE CORRECT COMMAND HERE\\n}\\nThe above JSON string states that first, the result is OK and the correctly formatted command.\\nThese commands are available:\\n- !transcribe [from-num-minutes-ago] (to-minutes-ago) // recall the transcription from X minutes ago up to Y minutes ago (Y is optional, defaults to 0).\\n- !transcribe [from-datetime] (to-datetime) // recall the transcription from the datetime X up to the datetime Y (Y is optional, defaults to now).\\n- !tsstart // start the transcription service\\n- !tsend // ends the transcription service\\n- !help // lists the available commands\\nA user is asking for: What time is it?\\nFind the relevant command and pass in the correct parameters. The parameters in these commands must be integers or dates. Do not leave plaintext in the commands.\\nUsing the relevant command and parameters, format a JSON string like we did above.\\n', 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 512, 'num_predict': 256}}\n",
      "To answer this user's question about the current time, we need to use the !help command with no additional parameters.\n",
      "\n",
      "Here is the corresponding JSON string:\n",
      "```\n",
      "{\n",
      "  \"result\": 200,\n",
      "  \"formatted-query\": \"!help\"\n",
      "}\n",
      "```\n",
      "Time taken: 2.587s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## HYBRID PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"What time is it?\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "PROMPT = \\\n",
    "\"\"\"\n",
    "Please return an JSON string using the following format:\n",
    "{\n",
    "\"result\": 200, // an HTTPS result, either 200 or 503.\n",
    "\"formatted-query\": \"!help\" // PUT THE CORRECT COMMAND HERE\n",
    "}\n",
    "The above JSON string states that first, the result is OK and the correctly formatted command.\n",
    "These commands are available:\n",
    "- !transcribe [from-num-minutes-ago] (to-minutes-ago) // recall the transcription from X minutes ago up to Y minutes ago (Y is optional, defaults to 0).\n",
    "- !transcribe [from-datetime] (to-datetime) // recall the transcription from the datetime X up to the datetime Y (Y is optional, defaults to now).\n",
    "- !tsstart // start the transcription service\n",
    "- !tsend // ends the transcription service\n",
    "- !help // lists the available commands\n",
    "A user is asking for: %s\n",
    "Find the relevant command and pass in the correct parameters. The parameters in these commands must be integers or dates. Do not leave plaintext in the commands.\n",
    "Using the relevant command and parameters, format a JSON string like we did above.\n",
    "\"\"\" % MESSAGE\n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=512, \n",
    "                         num_predict=256)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-eng-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
