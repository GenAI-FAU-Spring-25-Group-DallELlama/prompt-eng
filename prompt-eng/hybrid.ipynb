{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Prompting\n",
    "\n",
    "A combination of several different kinds of prompting.  This is our own contribution, but it is a fairly obvious follow-up to the other prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': '\\nA user is asking for: What was said in the last 10 minutes?\\nPlease return an JSON string using the following format:\\n{\\n\"result\": 200,\\n\"formatted-query\": \"!transcribe 5\"\\n}\\nThe above JSON string states that first, the result is OK (i.e. there exists a relevant command) and the correctly formatted command.\\nThese commands are available:\\n- !transcribe [from-num-minutes-ago] (to-minutes-ago)\\n- !transcribe [from-utc-time] (to-utc-time)\\n- !tsstart\\n- !tsend\\nThe parameters in these commands must be integers or dates. Do not leave plaintext in the commands.\\nOnly return the JSON string. Do not show your work and do not leave comments.\\n', 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 256, 'num_predict': 100}}\n",
      "{\n",
      "\"result\": 200,\n",
      "\"formatted-query\": \"!transcribe 10\"\n",
      "}\n",
      "Time taken: 1.135s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## HYBRID PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"What was said in the last 10 minutes?\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "PROMPT = \\\n",
    "\"\"\"\n",
    "A user is asking for: %s\n",
    "Please return an JSON string using the following format:\n",
    "{\n",
    "\"result\": 200,\n",
    "\"formatted-query\": \"!transcribe 5\"\n",
    "}\n",
    "The above JSON string states that first, the result is OK (i.e. there exists a relevant command) and the correctly formatted command.\n",
    "These commands are available:\n",
    "- !transcribe [from-num-minutes-ago] (to-minutes-ago)\n",
    "- !transcribe [from-utc-time] (to-utc-time)\n",
    "- !tsstart\n",
    "- !tsend\n",
    "The parameters in these commands must be integers or dates. Do not leave plaintext in the commands.\n",
    "Only return the JSON string. Do not show your work and do not leave comments.\n",
    "\"\"\" % MESSAGE\n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=256, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-eng-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
